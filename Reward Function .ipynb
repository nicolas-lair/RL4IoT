{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T16:59:44.178078Z",
     "start_time": "2020-07-22T16:59:44.158842Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T16:59:46.173446Z",
     "start_time": "2020-07-22T16:59:44.545125Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'src/')\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import joblib\n",
    "from torch import nn \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from architecture.reward import LearnedReward, StateDataset\n",
    "from architecture.language_model import LanguageModel\n",
    "from simulator.description_embedder import Description_embedder\n",
    "from simulator.Environment import preprocess_raw_observation\n",
    "from simulator.Items import ITEM_TYPE\n",
    "from config import generate_params\n",
    "\n",
    "params = generate_params(save_path=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T16:59:49.255323Z",
     "start_time": "2020-07-22T16:59:46.269413Z"
    }
   },
   "outputs": [],
   "source": [
    "# EpisodeRecord = namedtuple('EpisodeRecord', ('initial_state', 'final_state', 'instruction', 'reward'))\n",
    "# episode_path = 'results/episodes_records.jbl'\n",
    "# episodes = joblib.load(episode_path)\n",
    "\n",
    "StateRecord = namedtuple('StateRecord', ('state', 'instruction', 'reward'))\n",
    "state_path = 'results/state_records.jbl'\n",
    "states = joblib.load(state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:23:11.253026Z",
     "start_time": "2020-07-22T17:23:07.064081Z"
    }
   },
   "outputs": [],
   "source": [
    "description_embedder = Description_embedder(**params['env_params']['description_embedder_params'])\n",
    "\n",
    "item_type_embedder = OneHotEncoder(sparse=False)\n",
    "item_type_embedder.fit(np.array(ITEM_TYPE).reshape(-1, 1))\n",
    "\n",
    "from functools import partial\n",
    "transformer = partial(preprocess_raw_observation, description_embedder=description_embedder, item_type_embedder=item_type_embedder, raw_state_size=3, \n",
    "                      pytorch=True, device=params['device'])\n",
    "\n",
    "dts = StateDataset.from_files(state_path, raw_state_transformer=transformer)\n",
    "train_dts, test_dts = dts.split(train_test_ratio = 0.7)\n",
    "train_loader = DataLoader(train_dts, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:37:43.804559Z",
     "start_time": "2020-07-22T17:37:42.443385Z"
    }
   },
   "outputs": [],
   "source": [
    "language_model = LanguageModel(**params['language_model_params'])\n",
    "reward_function = LearnedReward(context_model=params['model_params']['context_model'], language_model=language_model, reward_params=params['reward_model_params'])\n",
    "reward_function.to(params['device'])\n",
    "\n",
    "from torch import optim\n",
    "loss_func = nn.BCELoss()\n",
    "optimizer = optim.Adam(reward_function.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:50:13.127538Z",
     "start_time": "2020-07-22T17:37:54.594935Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5558df2a6c3a4f3d93719b7b612caf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 loss: 0.5682401657104492\n",
      "0 1 loss: 0.5132606625556946\n",
      "0 2 loss: 0.4657959043979645\n",
      "0 3 loss: 0.4822753965854645\n",
      "0 4 loss: 0.4848179817199707\n",
      "0 5 loss: 0.5530427694320679\n",
      "0 6 loss: 0.47748324275016785\n",
      "0 7 loss: 0.46735668182373047\n",
      "0 8 loss: 0.5500054359436035\n",
      "0 9 loss: 0.4988878667354584\n",
      "0 10 loss: 0.5043541193008423\n",
      "0 11 loss: 0.5137562155723572\n",
      "0 12 loss: 0.47948575019836426\n",
      "0 13 loss: 0.5202174782752991\n",
      "0 14 loss: 0.5253622531890869\n",
      "0 15 loss: 0.45075851678848267\n",
      "0 16 loss: 0.5378974080085754\n",
      "0 17 loss: 0.5043112635612488\n",
      "0 18 loss: 0.420255184173584\n",
      "0 19 loss: 0.5173200964927673\n",
      "0 20 loss: 0.4945668876171112\n",
      "0 21 loss: 0.49150359630584717\n",
      "0 22 loss: 0.4637724459171295\n",
      "0 23 loss: 0.46485984325408936\n",
      "0 24 loss: 0.4032552242279053\n",
      "0 25 loss: 0.48541146516799927\n",
      "0 26 loss: 0.4103829860687256\n",
      "0 27 loss: 0.41680246591567993\n",
      "0 28 loss: 0.4197038412094116\n",
      "0 29 loss: 0.40884390473365784\n",
      "0 30 loss: 0.4138764441013336\n",
      "0 31 loss: 0.40121620893478394\n",
      "0 32 loss: 0.426405131816864\n",
      "0 33 loss: 0.45997461676597595\n",
      "0 34 loss: 0.38436102867126465\n",
      "0 35 loss: 0.4025682806968689\n",
      "0 36 loss: 0.4194071292877197\n",
      "0 37 loss: 0.38696005940437317\n",
      "0 38 loss: 0.4310814440250397\n",
      "0 39 loss: 0.43654125928878784\n",
      "0 40 loss: 0.38617274165153503\n",
      "0 41 loss: 0.4434237480163574\n",
      "0 42 loss: 0.40625321865081787\n",
      "0 43 loss: 0.41353660821914673\n",
      "0 44 loss: 0.38791099190711975\n",
      "0 45 loss: 0.3967832922935486\n",
      "0 46 loss: 0.3850994110107422\n",
      "0 47 loss: 0.4210558533668518\n",
      "0 48 loss: 0.43022316694259644\n",
      "0 49 loss: 0.36709946393966675\n",
      "0 50 loss: 0.33388760685920715\n",
      "0 51 loss: 0.41624748706817627\n",
      "0 52 loss: 0.4259611964225769\n",
      "0 53 loss: 0.3845600485801697\n",
      "0 54 loss: 0.3696078956127167\n",
      "0 55 loss: 0.39229583740234375\n",
      "0 56 loss: 0.39122334122657776\n",
      "0 57 loss: 0.3781587779521942\n",
      "0 58 loss: 0.4103695750236511\n",
      "0 59 loss: 0.4393079876899719\n",
      "0 60 loss: 0.4005762040615082\n",
      "0 61 loss: 0.3665042221546173\n",
      "0 62 loss: 0.3916199505329132\n",
      "0 63 loss: 0.36444076895713806\n",
      "0 64 loss: 0.3396471440792084\n",
      "0 65 loss: 0.41327103972435\n",
      "0 66 loss: 0.38983312249183655\n",
      "0 67 loss: 0.35561108589172363\n",
      "0 68 loss: 0.476813942193985\n",
      "0 69 loss: 0.37882548570632935\n",
      "0 70 loss: 0.34850484132766724\n",
      "0 71 loss: 0.37537771463394165\n",
      "0 72 loss: 0.40857669711112976\n",
      "0 73 loss: 0.37783315777778625\n",
      "0 74 loss: 0.38407549262046814\n",
      "0 75 loss: 0.39419203996658325\n",
      "0 76 loss: 0.3997809886932373\n",
      "0 77 loss: 0.3972359895706177\n",
      "0 78 loss: 0.43657660484313965\n",
      "0 79 loss: 0.37962204217910767\n",
      "0 80 loss: 0.34497490525245667\n",
      "0 81 loss: 0.44073718786239624\n",
      "0 82 loss: 0.43327122926712036\n",
      "0 83 loss: 0.4092353582382202\n",
      "0 84 loss: 0.37669992446899414\n",
      "0 85 loss: 0.3933403193950653\n",
      "0 86 loss: 0.335635781288147\n",
      "0 87 loss: 0.36382296681404114\n",
      "0 88 loss: 0.37874382734298706\n",
      "0 89 loss: 0.39607036113739014\n",
      "0 90 loss: 0.3779957890510559\n",
      "0 91 loss: 0.37310826778411865\n",
      "0 92 loss: 0.39405688643455505\n",
      "0 93 loss: 0.3519083857536316\n",
      "0 94 loss: 0.40908485651016235\n",
      "0 95 loss: 0.3525494337081909\n",
      "0 96 loss: 0.3788774907588959\n",
      "0 97 loss: 0.4031582176685333\n",
      "0 98 loss: 0.44040340185165405\n",
      "0 99 loss: 0.3660658597946167\n",
      "0 100 loss: 0.4445454478263855\n",
      "0 101 loss: 0.37227171659469604\n",
      "0 102 loss: 0.3935340642929077\n",
      "0 103 loss: 0.3408105969429016\n",
      "0 104 loss: 0.40955376625061035\n",
      "0 105 loss: 0.42631155252456665\n",
      "0 106 loss: 0.4116120934486389\n",
      "0 107 loss: 0.35312145948410034\n",
      "0 108 loss: 0.3755129873752594\n",
      "0 109 loss: 0.3101845681667328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b3cc7741924784a5ce7e22864d28dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 loss: 0.406681627035141\n",
      "1 1 loss: 0.36044466495513916\n",
      "1 2 loss: 0.342751681804657\n",
      "1 3 loss: 0.3575354218482971\n",
      "1 4 loss: 0.3565976619720459\n",
      "1 5 loss: 0.3453710377216339\n",
      "1 6 loss: 0.38615337014198303\n",
      "1 7 loss: 0.4458816647529602\n",
      "1 8 loss: 0.4053877592086792\n",
      "1 9 loss: 0.3635345697402954\n",
      "1 10 loss: 0.39844465255737305\n",
      "1 11 loss: 0.395344078540802\n",
      "1 12 loss: 0.4083905518054962\n",
      "1 13 loss: 0.38989365100860596\n",
      "1 14 loss: 0.42296743392944336\n",
      "1 15 loss: 0.3805938959121704\n",
      "1 16 loss: 0.38777345418930054\n",
      "1 17 loss: 0.3869689702987671\n",
      "1 18 loss: 0.3907890021800995\n",
      "1 19 loss: 0.3730151653289795\n",
      "1 20 loss: 0.3830687999725342\n",
      "1 21 loss: 0.40106552839279175\n",
      "1 22 loss: 0.4077830910682678\n",
      "1 23 loss: 0.44507309794425964\n",
      "1 24 loss: 0.33959200978279114\n",
      "1 25 loss: 0.4011821746826172\n",
      "1 26 loss: 0.4421408176422119\n",
      "1 27 loss: 0.3774830400943756\n",
      "1 28 loss: 0.41377198696136475\n",
      "1 29 loss: 0.35849234461784363\n",
      "1 30 loss: 0.41986238956451416\n",
      "1 31 loss: 0.39084649085998535\n",
      "1 32 loss: 0.33626294136047363\n",
      "1 33 loss: 0.34967565536499023\n",
      "1 34 loss: 0.41273659467697144\n",
      "1 35 loss: 0.3961777091026306\n",
      "1 36 loss: 0.27801063656806946\n",
      "1 37 loss: 0.3390005826950073\n",
      "1 38 loss: 0.39605712890625\n",
      "1 39 loss: 0.4365154206752777\n",
      "1 40 loss: 0.41497352719306946\n",
      "1 41 loss: 0.34378349781036377\n",
      "1 42 loss: 0.40846312046051025\n",
      "1 43 loss: 0.3177350163459778\n",
      "1 44 loss: 0.3860529661178589\n",
      "1 45 loss: 0.32914212346076965\n",
      "1 46 loss: 0.38270139694213867\n",
      "1 47 loss: 0.36877143383026123\n",
      "1 48 loss: 0.37561389803886414\n",
      "1 49 loss: 0.428970068693161\n",
      "1 50 loss: 0.35182467103004456\n",
      "1 51 loss: 0.3595442771911621\n",
      "1 52 loss: 0.2969505488872528\n",
      "1 53 loss: 0.32015499472618103\n",
      "1 54 loss: 0.3004438281059265\n",
      "1 55 loss: 0.42473363876342773\n",
      "1 56 loss: 0.35642266273498535\n",
      "1 57 loss: 0.3527126610279083\n",
      "1 58 loss: 0.41664376854896545\n",
      "1 59 loss: 0.34498006105422974\n",
      "1 60 loss: 0.4162634015083313\n",
      "1 61 loss: 0.38449281454086304\n",
      "1 62 loss: 0.43300315737724304\n",
      "1 63 loss: 0.3948914706707001\n",
      "1 64 loss: 0.3592718243598938\n",
      "1 65 loss: 0.37263041734695435\n",
      "1 66 loss: 0.40365660190582275\n",
      "1 67 loss: 0.40783238410949707\n",
      "1 68 loss: 0.4645878076553345\n",
      "1 69 loss: 0.4161832630634308\n",
      "1 70 loss: 0.39368656277656555\n",
      "1 71 loss: 0.40120983123779297\n",
      "1 72 loss: 0.34470415115356445\n",
      "1 73 loss: 0.3997458219528198\n",
      "1 74 loss: 0.35545799136161804\n",
      "1 75 loss: 0.4036717414855957\n",
      "1 76 loss: 0.3728798031806946\n",
      "1 77 loss: 0.3370248079299927\n",
      "1 78 loss: 0.4010367691516876\n",
      "1 79 loss: 0.350261926651001\n",
      "1 80 loss: 0.3598087430000305\n",
      "1 81 loss: 0.3952452838420868\n",
      "1 82 loss: 0.3934510052204132\n",
      "1 83 loss: 0.43827301263809204\n",
      "1 84 loss: 0.4113669991493225\n",
      "1 85 loss: 0.39507612586021423\n",
      "1 86 loss: 0.3885166049003601\n",
      "1 87 loss: 0.32620102167129517\n",
      "1 88 loss: 0.35213810205459595\n",
      "1 89 loss: 0.38218316435813904\n",
      "1 90 loss: 0.3386930823326111\n",
      "1 91 loss: 0.3445857763290405\n",
      "1 92 loss: 0.3385877013206482\n",
      "1 93 loss: 0.37967973947525024\n",
      "1 94 loss: 0.37442922592163086\n",
      "1 95 loss: 0.41344553232192993\n",
      "1 96 loss: 0.3839890956878662\n",
      "1 97 loss: 0.379021555185318\n",
      "1 98 loss: 0.3468888998031616\n",
      "1 99 loss: 0.3582078218460083\n",
      "1 100 loss: 0.3673914074897766\n",
      "1 101 loss: 0.3795014023780823\n",
      "1 102 loss: 0.35848918557167053\n",
      "1 103 loss: 0.356426477432251\n",
      "1 104 loss: 0.44170475006103516\n",
      "1 105 loss: 0.44666415452957153\n",
      "1 106 loss: 0.348840594291687\n",
      "1 107 loss: 0.29532307386398315\n",
      "1 108 loss: 0.4225124716758728\n",
      "1 109 loss: 0.3011144995689392\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcea78d44944d068947084557cfd211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0 loss: 0.406322717666626\n",
      "2 1 loss: 0.361453115940094\n",
      "2 2 loss: 0.3976450264453888\n",
      "2 3 loss: 0.3686724305152893\n",
      "2 4 loss: 0.4103855490684509\n",
      "2 5 loss: 0.3290349841117859\n",
      "2 6 loss: 0.3173905611038208\n",
      "2 7 loss: 0.3631643056869507\n",
      "2 8 loss: 0.3873271346092224\n",
      "2 9 loss: 0.3240271210670471\n",
      "2 10 loss: 0.43186697363853455\n",
      "2 11 loss: 0.3276539444923401\n",
      "2 12 loss: 0.341133713722229\n",
      "2 13 loss: 0.3940277099609375\n",
      "2 14 loss: 0.33957821130752563\n",
      "2 15 loss: 0.3831043839454651\n",
      "2 16 loss: 0.42106202244758606\n",
      "2 17 loss: 0.40726611018180847\n",
      "2 18 loss: 0.359681099653244\n",
      "2 19 loss: 0.3943941593170166\n",
      "2 20 loss: 0.3512236773967743\n",
      "2 21 loss: 0.39220646023750305\n",
      "2 22 loss: 0.3627638816833496\n",
      "2 23 loss: 0.3966241478919983\n",
      "2 24 loss: 0.2965431213378906\n",
      "2 25 loss: 0.4009533226490021\n",
      "2 26 loss: 0.4137839078903198\n",
      "2 27 loss: 0.3347840905189514\n",
      "2 28 loss: 0.3774317800998688\n",
      "2 29 loss: 0.36957883834838867\n",
      "2 30 loss: 0.3518381118774414\n",
      "2 31 loss: 0.3648398220539093\n",
      "2 32 loss: 0.3820677399635315\n",
      "2 33 loss: 0.3775951862335205\n",
      "2 34 loss: 0.44447144865989685\n",
      "2 35 loss: 0.33047300577163696\n",
      "2 36 loss: 0.4253363609313965\n",
      "2 37 loss: 0.32924720644950867\n",
      "2 38 loss: 0.4171629846096039\n",
      "2 39 loss: 0.38196974992752075\n",
      "2 40 loss: 0.3406931757926941\n",
      "2 41 loss: 0.3617270886898041\n",
      "2 42 loss: 0.3891172409057617\n",
      "2 43 loss: 0.37015897035598755\n",
      "2 44 loss: 0.4008241891860962\n",
      "2 45 loss: 0.3588651716709137\n",
      "2 46 loss: 0.37264347076416016\n",
      "2 47 loss: 0.4218807816505432\n",
      "2 48 loss: 0.39483487606048584\n",
      "2 49 loss: 0.36102503538131714\n",
      "2 50 loss: 0.41893553733825684\n",
      "2 51 loss: 0.33630502223968506\n",
      "2 52 loss: 0.39111876487731934\n",
      "2 53 loss: 0.3624177575111389\n",
      "2 54 loss: 0.40303876996040344\n",
      "2 55 loss: 0.40839117765426636\n",
      "2 56 loss: 0.3888721466064453\n",
      "2 57 loss: 0.3762866258621216\n",
      "2 58 loss: 0.3807377219200134\n",
      "2 59 loss: 0.4024704694747925\n",
      "2 60 loss: 0.37317872047424316\n",
      "2 61 loss: 0.3523162007331848\n",
      "2 62 loss: 0.397702693939209\n",
      "2 63 loss: 0.44149845838546753\n",
      "2 64 loss: 0.35846149921417236\n",
      "2 65 loss: 0.31665337085723877\n",
      "2 66 loss: 0.3945440948009491\n",
      "2 67 loss: 0.3272606134414673\n",
      "2 68 loss: 0.38396507501602173\n",
      "2 69 loss: 0.3893999457359314\n",
      "2 70 loss: 0.34790587425231934\n",
      "2 71 loss: 0.3954569101333618\n",
      "2 72 loss: 0.3585836887359619\n",
      "2 73 loss: 0.3634231686592102\n",
      "2 74 loss: 0.42596152424812317\n",
      "2 75 loss: 0.36315974593162537\n",
      "2 76 loss: 0.4194658696651459\n",
      "2 77 loss: 0.4108039140701294\n",
      "2 78 loss: 0.3356044590473175\n",
      "2 79 loss: 0.38866180181503296\n",
      "2 80 loss: 0.28864407539367676\n",
      "2 81 loss: 0.44184088706970215\n",
      "2 82 loss: 0.3810734748840332\n",
      "2 83 loss: 0.3651127517223358\n",
      "2 84 loss: 0.3573850691318512\n",
      "2 85 loss: 0.3848649859428406\n",
      "2 86 loss: 0.37260758876800537\n",
      "2 87 loss: 0.4114411771297455\n",
      "2 88 loss: 0.36547255516052246\n",
      "2 89 loss: 0.3561745882034302\n",
      "2 90 loss: 0.40824276208877563\n",
      "2 91 loss: 0.3558031916618347\n",
      "2 92 loss: 0.37115949392318726\n",
      "2 93 loss: 0.3806627690792084\n",
      "2 94 loss: 0.3741389811038971\n",
      "2 95 loss: 0.4305867552757263\n",
      "2 96 loss: 0.35733863711357117\n",
      "2 97 loss: 0.3216698169708252\n",
      "2 98 loss: 0.34147652983665466\n",
      "2 99 loss: 0.4410584270954132\n",
      "2 100 loss: 0.41124722361564636\n",
      "2 101 loss: 0.38077718019485474\n",
      "2 102 loss: 0.35179775953292847\n",
      "2 103 loss: 0.3602636456489563\n",
      "2 104 loss: 0.4179736375808716\n",
      "2 105 loss: 0.37298083305358887\n",
      "2 106 loss: 0.32674700021743774\n",
      "2 107 loss: 0.3224845826625824\n",
      "2 108 loss: 0.40498942136764526\n",
      "2 109 loss: 0.4124584197998047\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa32591ac5c44b61a62c46922eaad1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0 loss: 0.41743969917297363\n",
      "3 1 loss: 0.36490148305892944\n",
      "3 2 loss: 0.375455766916275\n",
      "3 3 loss: 0.3487674593925476\n",
      "3 4 loss: 0.4388711452484131\n",
      "3 5 loss: 0.38329941034317017\n",
      "3 6 loss: 0.3524816334247589\n",
      "3 7 loss: 0.4176784157752991\n",
      "3 8 loss: 0.39240550994873047\n",
      "3 9 loss: 0.3692868947982788\n",
      "3 10 loss: 0.3748292326927185\n",
      "3 11 loss: 0.381878137588501\n",
      "3 12 loss: 0.39503708481788635\n",
      "3 13 loss: 0.35754555463790894\n",
      "3 14 loss: 0.3726215958595276\n",
      "3 15 loss: 0.3773512840270996\n",
      "3 16 loss: 0.3642958998680115\n",
      "3 17 loss: 0.3767545223236084\n",
      "3 18 loss: 0.34136298298835754\n",
      "3 19 loss: 0.34937578439712524\n",
      "3 20 loss: 0.3952174782752991\n",
      "3 21 loss: 0.31669872999191284\n",
      "3 22 loss: 0.39603400230407715\n",
      "3 23 loss: 0.3526492118835449\n",
      "3 24 loss: 0.39151403307914734\n",
      "3 25 loss: 0.3594387471675873\n",
      "3 26 loss: 0.342756062746048\n",
      "3 27 loss: 0.3600562810897827\n",
      "3 28 loss: 0.3777998685836792\n",
      "3 29 loss: 0.4210238754749298\n",
      "3 30 loss: 0.3767186403274536\n",
      "3 31 loss: 0.4451233148574829\n",
      "3 32 loss: 0.35152798891067505\n",
      "3 33 loss: 0.39660966396331787\n",
      "3 34 loss: 0.4062674641609192\n",
      "3 35 loss: 0.4250214099884033\n",
      "3 36 loss: 0.350675106048584\n",
      "3 37 loss: 0.3649194538593292\n",
      "3 38 loss: 0.3699527978897095\n",
      "3 39 loss: 0.33975332975387573\n",
      "3 40 loss: 0.3595549464225769\n",
      "3 41 loss: 0.3766774535179138\n",
      "3 42 loss: 0.3103064000606537\n",
      "3 43 loss: 0.39077380299568176\n",
      "3 44 loss: 0.37935513257980347\n",
      "3 45 loss: 0.457561731338501\n",
      "3 46 loss: 0.4239718019962311\n",
      "3 47 loss: 0.348541796207428\n",
      "3 48 loss: 0.31982219219207764\n",
      "3 49 loss: 0.3818057179450989\n",
      "3 50 loss: 0.3193824887275696\n",
      "3 51 loss: 0.30021488666534424\n",
      "3 52 loss: 0.3926191031932831\n",
      "3 53 loss: 0.35331737995147705\n",
      "3 54 loss: 0.3902619779109955\n",
      "3 55 loss: 0.3871566653251648\n",
      "3 56 loss: 0.37312477827072144\n",
      "3 57 loss: 0.33502086997032166\n",
      "3 58 loss: 0.38545334339141846\n",
      "3 59 loss: 0.4641428589820862\n",
      "3 60 loss: 0.3412793278694153\n",
      "3 61 loss: 0.38802391290664673\n",
      "3 62 loss: 0.3410537838935852\n",
      "3 63 loss: 0.4163501560688019\n",
      "3 64 loss: 0.4311772584915161\n",
      "3 65 loss: 0.37252485752105713\n",
      "3 66 loss: 0.3547060489654541\n",
      "3 67 loss: 0.3863334655761719\n",
      "3 68 loss: 0.4241686761379242\n",
      "3 69 loss: 0.356190025806427\n",
      "3 70 loss: 0.42223960161209106\n",
      "3 71 loss: 0.38388437032699585\n",
      "3 72 loss: 0.39809978008270264\n",
      "3 73 loss: 0.3921085298061371\n",
      "3 74 loss: 0.38147324323654175\n",
      "3 75 loss: 0.35317981243133545\n",
      "3 76 loss: 0.39023956656455994\n",
      "3 77 loss: 0.3501492440700531\n",
      "3 78 loss: 0.39153504371643066\n",
      "3 79 loss: 0.37149083614349365\n",
      "3 80 loss: 0.34177762269973755\n",
      "3 81 loss: 0.33790796995162964\n",
      "3 82 loss: 0.3479081392288208\n",
      "3 83 loss: 0.3851061761379242\n",
      "3 84 loss: 0.35570839047431946\n",
      "3 85 loss: 0.3721155524253845\n",
      "3 86 loss: 0.3766375780105591\n",
      "3 87 loss: 0.34255266189575195\n",
      "3 88 loss: 0.31372135877609253\n",
      "3 89 loss: 0.37771832942962646\n",
      "3 90 loss: 0.3815678358078003\n",
      "3 91 loss: 0.33698952198028564\n",
      "3 92 loss: 0.3655773997306824\n",
      "3 93 loss: 0.3453211188316345\n",
      "3 94 loss: 0.39979803562164307\n",
      "3 95 loss: 0.3476065993309021\n",
      "3 96 loss: 0.35187315940856934\n",
      "3 97 loss: 0.27382582426071167\n",
      "3 98 loss: 0.3711671829223633\n",
      "3 99 loss: 0.40071901679039\n",
      "3 100 loss: 0.34680721163749695\n",
      "3 101 loss: 0.35052481293678284\n",
      "3 102 loss: 0.3707892894744873\n",
      "3 103 loss: 0.37497657537460327\n",
      "3 104 loss: 0.36303240060806274\n",
      "3 105 loss: 0.3598923981189728\n",
      "3 106 loss: 0.3961211144924164\n",
      "3 107 loss: 0.3567792773246765\n",
      "3 108 loss: 0.3636188507080078\n",
      "3 109 loss: 0.3976428210735321\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc76a84a1d74a9cb8e095ae46f9a109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0 loss: 0.3569982349872589\n",
      "4 1 loss: 0.4205894470214844\n",
      "4 2 loss: 0.3788601756095886\n",
      "4 3 loss: 0.34850189089775085\n",
      "4 4 loss: 0.37481677532196045\n",
      "4 5 loss: 0.3853682279586792\n",
      "4 6 loss: 0.3479650020599365\n",
      "4 7 loss: 0.3703775703907013\n",
      "4 8 loss: 0.39955276250839233\n",
      "4 9 loss: 0.4232828915119171\n",
      "4 10 loss: 0.3930678069591522\n",
      "4 11 loss: 0.3623284101486206\n",
      "4 12 loss: 0.3642697334289551\n",
      "4 13 loss: 0.3440856337547302\n",
      "4 14 loss: 0.37871021032333374\n",
      "4 15 loss: 0.36363837122917175\n",
      "4 16 loss: 0.35875046253204346\n",
      "4 17 loss: 0.4060922861099243\n",
      "4 18 loss: 0.3459157943725586\n",
      "4 19 loss: 0.29861462116241455\n",
      "4 20 loss: 0.40580421686172485\n",
      "4 21 loss: 0.36498361825942993\n",
      "4 22 loss: 0.40584713220596313\n",
      "4 23 loss: 0.37629491090774536\n",
      "4 24 loss: 0.33721137046813965\n",
      "4 25 loss: 0.3134847581386566\n",
      "4 26 loss: 0.3263556957244873\n",
      "4 27 loss: 0.3896130323410034\n",
      "4 28 loss: 0.3344916105270386\n",
      "4 29 loss: 0.31390973925590515\n",
      "4 30 loss: 0.35979872941970825\n",
      "4 31 loss: 0.3432846665382385\n",
      "4 32 loss: 0.33071160316467285\n",
      "4 33 loss: 0.432178258895874\n",
      "4 34 loss: 0.37790408730506897\n",
      "4 35 loss: 0.3573128581047058\n",
      "4 36 loss: 0.38181233406066895\n",
      "4 37 loss: 0.38245102763175964\n",
      "4 38 loss: 0.3321845233440399\n",
      "4 39 loss: 0.3936168849468231\n",
      "4 40 loss: 0.39845943450927734\n",
      "4 41 loss: 0.3216288685798645\n",
      "4 42 loss: 0.39050793647766113\n",
      "4 43 loss: 0.3802194893360138\n",
      "4 44 loss: 0.4119655191898346\n",
      "4 45 loss: 0.3749675452709198\n",
      "4 46 loss: 0.38267290592193604\n",
      "4 47 loss: 0.3712557554244995\n",
      "4 48 loss: 0.3215811848640442\n",
      "4 49 loss: 0.3608425259590149\n",
      "4 50 loss: 0.377841979265213\n",
      "4 51 loss: 0.40077054500579834\n",
      "4 52 loss: 0.3706004321575165\n",
      "4 53 loss: 0.35773777961730957\n",
      "4 54 loss: 0.3435051739215851\n",
      "4 55 loss: 0.33603519201278687\n",
      "4 56 loss: 0.33873528242111206\n",
      "4 57 loss: 0.41987699270248413\n",
      "4 58 loss: 0.3688022494316101\n",
      "4 59 loss: 0.3369693458080292\n",
      "4 60 loss: 0.36803674697875977\n",
      "4 61 loss: 0.375819593667984\n",
      "4 62 loss: 0.359798401594162\n",
      "4 63 loss: 0.3109583854675293\n",
      "4 64 loss: 0.3556392788887024\n",
      "4 65 loss: 0.37712615728378296\n",
      "4 66 loss: 0.32590487599372864\n",
      "4 67 loss: 0.38011056184768677\n",
      "4 68 loss: 0.37378358840942383\n",
      "4 69 loss: 0.35746270418167114\n",
      "4 70 loss: 0.3408406972885132\n",
      "4 71 loss: 0.34239470958709717\n",
      "4 72 loss: 0.3161207139492035\n",
      "4 73 loss: 0.3895826041698456\n",
      "4 74 loss: 0.42524954676628113\n",
      "4 75 loss: 0.4248366355895996\n",
      "4 76 loss: 0.3570614159107208\n",
      "4 77 loss: 0.3735566735267639\n",
      "4 78 loss: 0.39085888862609863\n",
      "4 79 loss: 0.33442217111587524\n",
      "4 80 loss: 0.38530564308166504\n",
      "4 81 loss: 0.37637102603912354\n",
      "4 82 loss: 0.3506467640399933\n",
      "4 83 loss: 0.37434494495391846\n",
      "4 84 loss: 0.3536112308502197\n",
      "4 85 loss: 0.35858407616615295\n",
      "4 86 loss: 0.35480642318725586\n",
      "4 87 loss: 0.34672775864601135\n",
      "4 88 loss: 0.3734121322631836\n",
      "4 89 loss: 0.3919517397880554\n",
      "4 90 loss: 0.409772664308548\n",
      "4 91 loss: 0.3938466012477875\n",
      "4 92 loss: 0.3747800588607788\n",
      "4 93 loss: 0.35982030630111694\n",
      "4 94 loss: 0.32519420981407166\n",
      "4 95 loss: 0.3407812714576721\n",
      "4 96 loss: 0.3807889223098755\n",
      "4 97 loss: 0.3246021866798401\n",
      "4 98 loss: 0.35187625885009766\n",
      "4 99 loss: 0.3697853088378906\n",
      "4 100 loss: 0.3465764820575714\n",
      "4 101 loss: 0.335536390542984\n",
      "4 102 loss: 0.35967105627059937\n",
      "4 103 loss: 0.33992522954940796\n",
      "4 104 loss: 0.3715401291847229\n",
      "4 105 loss: 0.35890087485313416\n",
      "4 106 loss: 0.3956124484539032\n",
      "4 107 loss: 0.3816484808921814\n",
      "4 108 loss: 0.33280640840530396\n",
      "4 109 loss: 0.45785003900527954\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29551696e2747c1848dabfc5f2407a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0 loss: 0.37915629148483276\n",
      "5 1 loss: 0.34897753596305847\n",
      "5 2 loss: 0.3860705494880676\n",
      "5 3 loss: 0.35705432295799255\n",
      "5 4 loss: 0.31735679507255554\n",
      "5 5 loss: 0.4195184111595154\n",
      "5 6 loss: 0.3483886122703552\n",
      "5 7 loss: 0.4085160195827484\n",
      "5 8 loss: 0.33717048168182373\n",
      "5 9 loss: 0.35746335983276367\n",
      "5 10 loss: 0.2990233600139618\n",
      "5 11 loss: 0.34920552372932434\n",
      "5 12 loss: 0.40370678901672363\n",
      "5 13 loss: 0.3848256766796112\n",
      "5 14 loss: 0.3769650161266327\n",
      "5 15 loss: 0.3789311349391937\n",
      "5 16 loss: 0.32293781638145447\n",
      "5 17 loss: 0.3758222460746765\n",
      "5 18 loss: 0.3517743647098541\n",
      "5 19 loss: 0.35753530263900757\n",
      "5 20 loss: 0.3505092263221741\n",
      "5 21 loss: 0.39528951048851013\n",
      "5 22 loss: 0.33235645294189453\n",
      "5 23 loss: 0.28874728083610535\n",
      "5 24 loss: 0.4098295569419861\n",
      "5 25 loss: 0.43167591094970703\n",
      "5 26 loss: 0.343405157327652\n",
      "5 27 loss: 0.37978821992874146\n",
      "5 28 loss: 0.45976579189300537\n",
      "5 29 loss: 0.32797110080718994\n",
      "5 30 loss: 0.29566341638565063\n",
      "5 31 loss: 0.37572866678237915\n",
      "5 32 loss: 0.3478066325187683\n",
      "5 33 loss: 0.34294188022613525\n",
      "5 34 loss: 0.4079914391040802\n",
      "5 35 loss: 0.33825379610061646\n",
      "5 36 loss: 0.4385056495666504\n",
      "5 37 loss: 0.34307044744491577\n",
      "5 38 loss: 0.38542410731315613\n",
      "5 39 loss: 0.34229013323783875\n",
      "5 40 loss: 0.35244855284690857\n",
      "5 41 loss: 0.36844950914382935\n",
      "5 42 loss: 0.41676750779151917\n",
      "5 43 loss: 0.3569154143333435\n",
      "5 44 loss: 0.3297094702720642\n",
      "5 45 loss: 0.33386069536209106\n",
      "5 46 loss: 0.3494049310684204\n",
      "5 47 loss: 0.3884326219558716\n",
      "5 48 loss: 0.33660754561424255\n",
      "5 49 loss: 0.34948816895484924\n",
      "5 50 loss: 0.3434101939201355\n",
      "5 51 loss: 0.31198936700820923\n",
      "5 52 loss: 0.339824378490448\n",
      "5 53 loss: 0.40688174962997437\n",
      "5 54 loss: 0.3349945843219757\n",
      "5 55 loss: 0.2913520038127899\n",
      "5 56 loss: 0.38093775510787964\n",
      "5 57 loss: 0.38153135776519775\n",
      "5 58 loss: 0.36496320366859436\n",
      "5 59 loss: 0.37367120385169983\n",
      "5 60 loss: 0.3707507252693176\n",
      "5 61 loss: 0.34864675998687744\n",
      "5 62 loss: 0.3200661540031433\n",
      "5 63 loss: 0.3793908953666687\n",
      "5 64 loss: 0.3323069214820862\n",
      "5 65 loss: 0.4013766646385193\n",
      "5 66 loss: 0.3610321283340454\n",
      "5 67 loss: 0.35174643993377686\n",
      "5 68 loss: 0.3905766010284424\n",
      "5 69 loss: 0.30839425325393677\n",
      "5 70 loss: 0.2957007586956024\n",
      "5 71 loss: 0.36339616775512695\n",
      "5 72 loss: 0.40110886096954346\n",
      "5 73 loss: 0.3640720546245575\n",
      "5 74 loss: 0.3487986922264099\n",
      "5 75 loss: 0.3625892400741577\n",
      "5 76 loss: 0.3707547187805176\n",
      "5 77 loss: 0.346035361289978\n",
      "5 78 loss: 0.34080052375793457\n",
      "5 79 loss: 0.3539734482765198\n",
      "5 80 loss: 0.36966317892074585\n",
      "5 81 loss: 0.38523948192596436\n",
      "5 82 loss: 0.34792351722717285\n",
      "5 83 loss: 0.37414056062698364\n",
      "5 84 loss: 0.33449721336364746\n",
      "5 85 loss: 0.3634893596172333\n",
      "5 86 loss: 0.34681740403175354\n",
      "5 87 loss: 0.3238810896873474\n",
      "5 88 loss: 0.33981648087501526\n",
      "5 89 loss: 0.3389606475830078\n",
      "5 90 loss: 0.3879740238189697\n",
      "5 91 loss: 0.39308059215545654\n",
      "5 92 loss: 0.31556668877601624\n",
      "5 93 loss: 0.37241458892822266\n",
      "5 94 loss: 0.3516167104244232\n",
      "5 95 loss: 0.34127262234687805\n",
      "5 96 loss: 0.37658482789993286\n",
      "5 97 loss: 0.38528716564178467\n",
      "5 98 loss: 0.3407937288284302\n",
      "5 99 loss: 0.32256224751472473\n",
      "5 100 loss: 0.3543223738670349\n",
      "5 101 loss: 0.3891648054122925\n",
      "5 102 loss: 0.3050835132598877\n",
      "5 103 loss: 0.3507348597049713\n",
      "5 104 loss: 0.364665150642395\n",
      "5 105 loss: 0.38799339532852173\n",
      "5 106 loss: 0.2984374761581421\n",
      "5 107 loss: 0.313254714012146\n",
      "5 108 loss: 0.3493039309978485\n",
      "5 109 loss: 0.3788277506828308\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c144f83d64cb45dea932480ce135a6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0 loss: 0.3621733784675598\n",
      "6 1 loss: 0.38229334354400635\n",
      "6 2 loss: 0.34063178300857544\n",
      "6 3 loss: 0.2886096239089966\n",
      "6 4 loss: 0.37924280762672424\n",
      "6 5 loss: 0.3181087076663971\n",
      "6 6 loss: 0.3698352575302124\n",
      "6 7 loss: 0.30757442116737366\n",
      "6 8 loss: 0.28097066283226013\n",
      "6 9 loss: 0.360700786113739\n",
      "6 10 loss: 0.3274194002151489\n",
      "6 11 loss: 0.38686585426330566\n",
      "6 12 loss: 0.3328765332698822\n",
      "6 13 loss: 0.37430810928344727\n",
      "6 14 loss: 0.3336579203605652\n",
      "6 15 loss: 0.34255141019821167\n",
      "6 16 loss: 0.3310132920742035\n",
      "6 17 loss: 0.3247770667076111\n",
      "6 18 loss: 0.38664913177490234\n",
      "6 19 loss: 0.3423462510108948\n",
      "6 20 loss: 0.2838042974472046\n",
      "6 21 loss: 0.27749747037887573\n",
      "6 22 loss: 0.3270137310028076\n",
      "6 23 loss: 0.38383948802948\n",
      "6 24 loss: 0.31187674403190613\n",
      "6 25 loss: 0.36040085554122925\n",
      "6 26 loss: 0.3435211181640625\n",
      "6 27 loss: 0.3252468705177307\n",
      "6 28 loss: 0.37531939148902893\n",
      "6 29 loss: 0.338949978351593\n",
      "6 30 loss: 0.3573147654533386\n",
      "6 31 loss: 0.3752945065498352\n",
      "6 32 loss: 0.28784069418907166\n",
      "6 33 loss: 0.3642634153366089\n",
      "6 34 loss: 0.41551798582077026\n",
      "6 35 loss: 0.32840171456336975\n",
      "6 36 loss: 0.32412827014923096\n",
      "6 37 loss: 0.4268489480018616\n",
      "6 38 loss: 0.3267667293548584\n",
      "6 39 loss: 0.36496424674987793\n",
      "6 40 loss: 0.3560231924057007\n",
      "6 41 loss: 0.33888334035873413\n",
      "6 42 loss: 0.27793824672698975\n",
      "6 43 loss: 0.37194448709487915\n",
      "6 44 loss: 0.38845276832580566\n",
      "6 45 loss: 0.3236803710460663\n",
      "6 46 loss: 0.34403377771377563\n",
      "6 47 loss: 0.4074035584926605\n",
      "6 48 loss: 0.29098397493362427\n",
      "6 49 loss: 0.3562021553516388\n",
      "6 50 loss: 0.3516682982444763\n",
      "6 51 loss: 0.3622168004512787\n",
      "6 52 loss: 0.3502211570739746\n",
      "6 53 loss: 0.35874319076538086\n",
      "6 54 loss: 0.35783055424690247\n",
      "6 55 loss: 0.3099643886089325\n",
      "6 56 loss: 0.3199140429496765\n",
      "6 57 loss: 0.36526674032211304\n",
      "6 58 loss: 0.3481230139732361\n",
      "6 59 loss: 0.3833059072494507\n",
      "6 60 loss: 0.3565883934497833\n",
      "6 61 loss: 0.32348379492759705\n",
      "6 62 loss: 0.27974945306777954\n",
      "6 63 loss: 0.373653382062912\n",
      "6 64 loss: 0.3714342415332794\n",
      "6 65 loss: 0.3115563988685608\n",
      "6 66 loss: 0.35532906651496887\n",
      "6 67 loss: 0.3373454809188843\n",
      "6 68 loss: 0.31396734714508057\n",
      "6 69 loss: 0.37534791231155396\n",
      "6 70 loss: 0.3110831379890442\n",
      "6 71 loss: 0.33212268352508545\n",
      "6 72 loss: 0.3200049102306366\n",
      "6 73 loss: 0.3424968123435974\n",
      "6 74 loss: 0.3647432327270508\n",
      "6 75 loss: 0.38012880086898804\n",
      "6 76 loss: 0.3522126078605652\n",
      "6 77 loss: 0.3182447552680969\n",
      "6 78 loss: 0.3438960313796997\n",
      "6 79 loss: 0.3180966377258301\n",
      "6 80 loss: 0.3477729558944702\n",
      "6 81 loss: 0.3461553752422333\n",
      "6 82 loss: 0.3124728500843048\n",
      "6 83 loss: 0.34631049633026123\n",
      "6 84 loss: 0.3786936402320862\n",
      "6 85 loss: 0.30179041624069214\n",
      "6 86 loss: 0.35514020919799805\n",
      "6 87 loss: 0.30899009108543396\n",
      "6 88 loss: 0.32397833466529846\n",
      "6 89 loss: 0.32323217391967773\n",
      "6 90 loss: 0.29275715351104736\n",
      "6 91 loss: 0.3497620224952698\n",
      "6 92 loss: 0.3819916546344757\n",
      "6 93 loss: 0.31542545557022095\n",
      "6 94 loss: 0.2974250912666321\n",
      "6 95 loss: 0.346286803483963\n",
      "6 96 loss: 0.3253094553947449\n",
      "6 97 loss: 0.32478201389312744\n",
      "6 98 loss: 0.3298478126525879\n",
      "6 99 loss: 0.3121451139450073\n",
      "6 100 loss: 0.3540183901786804\n",
      "6 101 loss: 0.392261803150177\n",
      "6 102 loss: 0.37215563654899597\n",
      "6 103 loss: 0.3548387289047241\n",
      "6 104 loss: 0.38326436281204224\n",
      "6 105 loss: 0.3387337028980255\n",
      "6 106 loss: 0.3287026286125183\n",
      "6 107 loss: 0.33238694071769714\n",
      "6 108 loss: 0.3665570616722107\n",
      "6 109 loss: 0.2964838445186615\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8b9fd4f71240f69d4050b47760a71a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0 loss: 0.3805643618106842\n",
      "7 1 loss: 0.31953924894332886\n",
      "7 2 loss: 0.3296276926994324\n",
      "7 3 loss: 0.35012146830558777\n",
      "7 4 loss: 0.3149760365486145\n",
      "7 5 loss: 0.36164194345474243\n",
      "7 6 loss: 0.3063359260559082\n",
      "7 7 loss: 0.3722578287124634\n",
      "7 8 loss: 0.2997961640357971\n",
      "7 9 loss: 0.3291665017604828\n",
      "7 10 loss: 0.35397768020629883\n",
      "7 11 loss: 0.2996962070465088\n",
      "7 12 loss: 0.35669851303100586\n",
      "7 13 loss: 0.31830310821533203\n",
      "7 14 loss: 0.3547627925872803\n",
      "7 15 loss: 0.29586154222488403\n",
      "7 16 loss: 0.3248778283596039\n",
      "7 17 loss: 0.3043077290058136\n",
      "7 18 loss: 0.31467610597610474\n",
      "7 19 loss: 0.37043240666389465\n",
      "7 20 loss: 0.36720362305641174\n",
      "7 21 loss: 0.366363525390625\n",
      "7 22 loss: 0.3178001642227173\n",
      "7 23 loss: 0.34349173307418823\n",
      "7 24 loss: 0.3817111551761627\n",
      "7 25 loss: 0.3425638675689697\n",
      "7 26 loss: 0.32829010486602783\n",
      "7 27 loss: 0.30875444412231445\n",
      "7 28 loss: 0.3690110146999359\n",
      "7 29 loss: 0.31950390338897705\n",
      "7 30 loss: 0.3441549837589264\n",
      "7 31 loss: 0.34848976135253906\n",
      "7 32 loss: 0.2751099467277527\n",
      "7 33 loss: 0.29864034056663513\n",
      "7 34 loss: 0.3315791189670563\n",
      "7 35 loss: 0.4177453815937042\n",
      "7 36 loss: 0.30278903245925903\n",
      "7 37 loss: 0.2998065948486328\n",
      "7 38 loss: 0.35158708691596985\n",
      "7 39 loss: 0.30335497856140137\n",
      "7 40 loss: 0.3463243246078491\n",
      "7 41 loss: 0.3161693513393402\n",
      "7 42 loss: 0.30363237857818604\n",
      "7 43 loss: 0.36015549302101135\n",
      "7 44 loss: 0.38300424814224243\n",
      "7 45 loss: 0.3476178050041199\n",
      "7 46 loss: 0.3452478051185608\n",
      "7 47 loss: 0.3624359965324402\n",
      "7 48 loss: 0.3076998293399811\n",
      "7 49 loss: 0.3419489860534668\n",
      "7 50 loss: 0.3393211364746094\n",
      "7 51 loss: 0.31565728783607483\n",
      "7 52 loss: 0.3205452561378479\n",
      "7 53 loss: 0.3090146481990814\n",
      "7 54 loss: 0.3449470102787018\n",
      "7 55 loss: 0.29472413659095764\n",
      "7 56 loss: 0.29734474420547485\n",
      "7 57 loss: 0.33303302526474\n",
      "7 58 loss: 0.3888198733329773\n",
      "7 59 loss: 0.329264372587204\n",
      "7 60 loss: 0.31332483887672424\n",
      "7 61 loss: 0.3705936670303345\n",
      "7 62 loss: 0.32471707463264465\n",
      "7 63 loss: 0.2742658853530884\n",
      "7 64 loss: 0.3737499713897705\n",
      "7 65 loss: 0.36870303750038147\n",
      "7 66 loss: 0.3644343912601471\n",
      "7 67 loss: 0.2833792269229889\n",
      "7 68 loss: 0.3431018590927124\n",
      "7 69 loss: 0.3141336739063263\n",
      "7 70 loss: 0.28617435693740845\n",
      "7 71 loss: 0.3884783387184143\n",
      "7 72 loss: 0.3442401885986328\n",
      "7 73 loss: 0.35773810744285583\n",
      "7 74 loss: 0.3132575750350952\n",
      "7 75 loss: 0.32662734389305115\n",
      "7 76 loss: 0.3219488859176636\n",
      "7 77 loss: 0.3661574721336365\n",
      "7 78 loss: 0.38895517587661743\n",
      "7 79 loss: 0.32881852984428406\n",
      "7 80 loss: 0.3354884684085846\n",
      "7 81 loss: 0.32378384470939636\n",
      "7 82 loss: 0.3336387276649475\n",
      "7 83 loss: 0.2920754551887512\n",
      "7 84 loss: 0.3305268883705139\n",
      "7 85 loss: 0.3187887668609619\n",
      "7 86 loss: 0.3141079545021057\n",
      "7 87 loss: 0.31411534547805786\n",
      "7 88 loss: 0.39550572633743286\n",
      "7 89 loss: 0.2919158935546875\n",
      "7 90 loss: 0.35832923650741577\n",
      "7 91 loss: 0.2834129333496094\n",
      "7 92 loss: 0.3086060881614685\n",
      "7 93 loss: 0.3126422166824341\n",
      "7 94 loss: 0.32098740339279175\n",
      "7 95 loss: 0.31651389598846436\n",
      "7 96 loss: 0.3350819945335388\n",
      "7 97 loss: 0.32707977294921875\n",
      "7 98 loss: 0.3381192088127136\n",
      "7 99 loss: 0.37249982357025146\n",
      "7 100 loss: 0.364810049533844\n",
      "7 101 loss: 0.3484295904636383\n",
      "7 102 loss: 0.28678685426712036\n",
      "7 103 loss: 0.3524751663208008\n",
      "7 104 loss: 0.3996751308441162\n",
      "7 105 loss: 0.3551214337348938\n",
      "7 106 loss: 0.30841439962387085\n",
      "7 107 loss: 0.3618529438972473\n",
      "7 108 loss: 0.33987706899642944\n",
      "7 109 loss: 0.38095641136169434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e40fba04b34a45b617ed053a65f02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0 loss: 0.30048683285713196\n",
      "8 1 loss: 0.37082818150520325\n",
      "8 2 loss: 0.29001545906066895\n",
      "8 3 loss: 0.3106275796890259\n",
      "8 4 loss: 0.3366279602050781\n",
      "8 5 loss: 0.2907494902610779\n",
      "8 6 loss: 0.3097574710845947\n",
      "8 7 loss: 0.362316370010376\n",
      "8 8 loss: 0.32791799306869507\n",
      "8 9 loss: 0.36737456917762756\n",
      "8 10 loss: 0.34854722023010254\n",
      "8 11 loss: 0.3819909691810608\n",
      "8 12 loss: 0.333928644657135\n",
      "8 13 loss: 0.3450016677379608\n",
      "8 14 loss: 0.32752180099487305\n",
      "8 15 loss: 0.31569811701774597\n",
      "8 16 loss: 0.2623787224292755\n",
      "8 17 loss: 0.3064344525337219\n",
      "8 18 loss: 0.3492897152900696\n",
      "8 19 loss: 0.30059802532196045\n",
      "8 20 loss: 0.3030965328216553\n",
      "8 21 loss: 0.3620145916938782\n",
      "8 22 loss: 0.3519837260246277\n",
      "8 23 loss: 0.39854392409324646\n",
      "8 24 loss: 0.38315248489379883\n",
      "8 25 loss: 0.3173547387123108\n",
      "8 26 loss: 0.3332812190055847\n",
      "8 27 loss: 0.37729108333587646\n",
      "8 28 loss: 0.3415592908859253\n",
      "8 29 loss: 0.31385117769241333\n",
      "8 30 loss: 0.3237370550632477\n",
      "8 31 loss: 0.3344658613204956\n",
      "8 32 loss: 0.3422316908836365\n",
      "8 33 loss: 0.32022160291671753\n",
      "8 34 loss: 0.3286122977733612\n",
      "8 35 loss: 0.3558427393436432\n",
      "8 36 loss: 0.3014470040798187\n",
      "8 37 loss: 0.2988600730895996\n",
      "8 38 loss: 0.33454522490501404\n",
      "8 39 loss: 0.28752392530441284\n",
      "8 40 loss: 0.3161972761154175\n",
      "8 41 loss: 0.32843971252441406\n",
      "8 42 loss: 0.3462619185447693\n",
      "8 43 loss: 0.3202432692050934\n",
      "8 44 loss: 0.34467414021492004\n",
      "8 45 loss: 0.3320443034172058\n",
      "8 46 loss: 0.37862473726272583\n",
      "8 47 loss: 0.3012921214103699\n",
      "8 48 loss: 0.3337187170982361\n",
      "8 49 loss: 0.29038968682289124\n",
      "8 50 loss: 0.33362334966659546\n",
      "8 51 loss: 0.36752957105636597\n",
      "8 52 loss: 0.3515337109565735\n",
      "8 53 loss: 0.35359466075897217\n",
      "8 54 loss: 0.30148929357528687\n",
      "8 55 loss: 0.3344793915748596\n",
      "8 56 loss: 0.32190755009651184\n",
      "8 57 loss: 0.26902246475219727\n",
      "8 58 loss: 0.3091517686843872\n",
      "8 59 loss: 0.3467187285423279\n",
      "8 60 loss: 0.30462899804115295\n",
      "8 61 loss: 0.39555492997169495\n",
      "8 62 loss: 0.3101480007171631\n",
      "8 63 loss: 0.3020689785480499\n",
      "8 64 loss: 0.29684603214263916\n",
      "8 65 loss: 0.2886996269226074\n",
      "8 66 loss: 0.30175280570983887\n",
      "8 67 loss: 0.36580678820610046\n",
      "8 68 loss: 0.27613046765327454\n",
      "8 69 loss: 0.3061487376689911\n",
      "8 70 loss: 0.2797926962375641\n",
      "8 71 loss: 0.3805416524410248\n",
      "8 72 loss: 0.381422221660614\n",
      "8 73 loss: 0.3587917983531952\n",
      "8 74 loss: 0.38093262910842896\n",
      "8 75 loss: 0.37605559825897217\n",
      "8 76 loss: 0.31883206963539124\n",
      "8 77 loss: 0.3790600299835205\n",
      "8 78 loss: 0.35435569286346436\n",
      "8 79 loss: 0.33930179476737976\n",
      "8 80 loss: 0.33251041173934937\n",
      "8 81 loss: 0.3286077380180359\n",
      "8 82 loss: 0.3372187316417694\n",
      "8 83 loss: 0.34795910120010376\n",
      "8 84 loss: 0.3183545470237732\n",
      "8 85 loss: 0.31223243474960327\n",
      "8 86 loss: 0.3722184896469116\n",
      "8 87 loss: 0.3368390202522278\n",
      "8 88 loss: 0.3928646445274353\n",
      "8 89 loss: 0.36616837978363037\n",
      "8 90 loss: 0.3300051689147949\n",
      "8 91 loss: 0.3445669114589691\n",
      "8 92 loss: 0.2862849831581116\n",
      "8 93 loss: 0.2940862774848938\n",
      "8 94 loss: 0.3122153878211975\n",
      "8 95 loss: 0.3700786232948303\n",
      "8 96 loss: 0.302200585603714\n",
      "8 97 loss: 0.32684797048568726\n",
      "8 98 loss: 0.33299559354782104\n",
      "8 99 loss: 0.2647191882133484\n",
      "8 100 loss: 0.3476281464099884\n",
      "8 101 loss: 0.31991589069366455\n",
      "8 102 loss: 0.3425776958465576\n",
      "8 103 loss: 0.31799840927124023\n",
      "8 104 loss: 0.3662858009338379\n",
      "8 105 loss: 0.31149953603744507\n",
      "8 106 loss: 0.3621150851249695\n",
      "8 107 loss: 0.3610546588897705\n",
      "8 108 loss: 0.3122994899749756\n",
      "8 109 loss: 0.30291932821273804\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-b50df467556a>:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i, batch in tqdm_notebook(enumerate(train_loader)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3391e27b9e804c738cf7e185813a52c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0 loss: 0.2647171914577484\n",
      "9 1 loss: 0.3124886155128479\n",
      "9 2 loss: 0.33709877729415894\n",
      "9 3 loss: 0.3129734992980957\n",
      "9 4 loss: 0.3529457151889801\n",
      "9 5 loss: 0.32052797079086304\n",
      "9 6 loss: 0.3315727412700653\n",
      "9 7 loss: 0.33466845750808716\n",
      "9 8 loss: 0.34592166543006897\n",
      "9 9 loss: 0.31429362297058105\n",
      "9 10 loss: 0.33470308780670166\n",
      "9 11 loss: 0.33832070231437683\n",
      "9 12 loss: 0.36328795552253723\n",
      "9 13 loss: 0.27867478132247925\n",
      "9 14 loss: 0.36596521735191345\n",
      "9 15 loss: 0.34073591232299805\n",
      "9 16 loss: 0.28056344389915466\n",
      "9 17 loss: 0.3333919644355774\n",
      "9 18 loss: 0.3234431743621826\n",
      "9 19 loss: 0.3174780011177063\n",
      "9 20 loss: 0.2860029935836792\n",
      "9 21 loss: 0.32642239332199097\n",
      "9 22 loss: 0.3052043318748474\n",
      "9 23 loss: 0.296073853969574\n",
      "9 24 loss: 0.2764359712600708\n",
      "9 25 loss: 0.3330647349357605\n",
      "9 26 loss: 0.39731037616729736\n",
      "9 27 loss: 0.3184013366699219\n",
      "9 28 loss: 0.2779499888420105\n",
      "9 29 loss: 0.3335905075073242\n",
      "9 30 loss: 0.3071973919868469\n",
      "9 31 loss: 0.39387282729148865\n",
      "9 32 loss: 0.33152198791503906\n",
      "9 33 loss: 0.3379785716533661\n",
      "9 34 loss: 0.3632969856262207\n",
      "9 35 loss: 0.3265829384326935\n",
      "9 36 loss: 0.3062191605567932\n",
      "9 37 loss: 0.34026703238487244\n",
      "9 38 loss: 0.33653098344802856\n",
      "9 39 loss: 0.31072208285331726\n",
      "9 40 loss: 0.32828664779663086\n",
      "9 41 loss: 0.318759560585022\n",
      "9 42 loss: 0.3576831519603729\n",
      "9 43 loss: 0.3319762945175171\n",
      "9 44 loss: 0.3490489423274994\n",
      "9 45 loss: 0.33801618218421936\n",
      "9 46 loss: 0.34029680490493774\n",
      "9 47 loss: 0.3252321481704712\n",
      "9 48 loss: 0.3289584517478943\n",
      "9 49 loss: 0.3330264687538147\n",
      "9 50 loss: 0.31319504976272583\n",
      "9 51 loss: 0.32156163454055786\n",
      "9 52 loss: 0.33887144923210144\n",
      "9 53 loss: 0.3013400137424469\n",
      "9 54 loss: 0.4070592522621155\n",
      "9 55 loss: 0.26633912324905396\n",
      "9 56 loss: 0.32346779108047485\n",
      "9 57 loss: 0.32420557737350464\n",
      "9 58 loss: 0.3438602685928345\n",
      "9 59 loss: 0.3836703300476074\n",
      "9 60 loss: 0.3037290573120117\n",
      "9 61 loss: 0.3324766755104065\n",
      "9 62 loss: 0.3531249463558197\n",
      "9 63 loss: 0.3709375262260437\n",
      "9 64 loss: 0.36191850900650024\n",
      "9 65 loss: 0.33388593792915344\n",
      "9 66 loss: 0.3201759457588196\n",
      "9 67 loss: 0.3691904842853546\n",
      "9 68 loss: 0.34351837635040283\n",
      "9 69 loss: 0.30500614643096924\n",
      "9 70 loss: 0.34422630071640015\n",
      "9 71 loss: 0.38431912660598755\n",
      "9 72 loss: 0.3351820707321167\n",
      "9 73 loss: 0.32168638706207275\n",
      "9 74 loss: 0.337256520986557\n",
      "9 75 loss: 0.3444267213344574\n",
      "9 76 loss: 0.3306514024734497\n",
      "9 77 loss: 0.3671236038208008\n",
      "9 78 loss: 0.3442942500114441\n",
      "9 79 loss: 0.3344114422798157\n",
      "9 80 loss: 0.3434613347053528\n",
      "9 81 loss: 0.3330656886100769\n",
      "9 82 loss: 0.3255017399787903\n",
      "9 83 loss: 0.32607609033584595\n",
      "9 84 loss: 0.34631431102752686\n",
      "9 85 loss: 0.32037967443466187\n",
      "9 86 loss: 0.33229976892471313\n",
      "9 87 loss: 0.31717315316200256\n",
      "9 88 loss: 0.31341248750686646\n",
      "9 89 loss: 0.33460086584091187\n",
      "9 90 loss: 0.3190847337245941\n",
      "9 91 loss: 0.3107187747955322\n",
      "9 92 loss: 0.29720979928970337\n",
      "9 93 loss: 0.3091413974761963\n",
      "9 94 loss: 0.32474207878112793\n",
      "9 95 loss: 0.376854807138443\n",
      "9 96 loss: 0.34375959634780884\n",
      "9 97 loss: 0.296522855758667\n",
      "9 98 loss: 0.273625910282135\n",
      "9 99 loss: 0.34758222103118896\n",
      "9 100 loss: 0.3226456046104431\n",
      "9 101 loss: 0.34445634484291077\n",
      "9 102 loss: 0.304518461227417\n",
      "9 103 loss: 0.36689573526382446\n",
      "9 104 loss: 0.3182094097137451\n",
      "9 105 loss: 0.2873409390449524\n",
      "9 106 loss: 0.35334575176239014\n",
      "9 107 loss: 0.3599321246147156\n",
      "9 108 loss: 0.28584814071655273\n",
      "9 109 loss: 0.29522621631622314\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i, batch in tqdm_notebook(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        reward = reward_function(state=batch['state'], instructions=batch['instruction']).view(-1)\n",
    "        loss = loss_func(reward, batch['reward'].float().to(params['device']))\n",
    "        print(f'{epoch} {i} loss: {loss.item()}')\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(reward_function.parameters(), 1)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:35:34.684281Z",
     "start_time": "2020-07-22T17:35:34.653439Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True,  True, False, False, False,\n",
       "        False,  True, False,  True, False,  True, False, False, False, False,\n",
       "         True, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True, False,\n",
       "         True, False, False, False,  True, False, False, False,  True, False,\n",
       "        False,  True, False, False, False, False,  True,  True,  True,  True,\n",
       "        False, False,  True, False, False, False,  True, False, False,  True,\n",
       "        False, False, False, False, False,  True,  True, False, False, False,\n",
       "         True, False, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "         True, False, False, False, False, False, False, False, False, False,\n",
       "        False,  True,  True, False, False, False, False,  True, False, False,\n",
       "        False, False,  True, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True,  True,\n",
       "        False, False, False,  True, False,  True, False,  True, False, False,\n",
       "        False, False, False, False, False,  True,  True,  True,  True, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "         True,  True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False, False,\n",
       "        False, False,  True, False,  True, False, False, False,  True, False,\n",
       "        False,  True, False,  True,  True, False, False, False, False, False,\n",
       "        False, False,  True, False, False, False])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['reward']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:35:40.455491Z",
     "start_time": "2020-07-22T17:35:40.415637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3085, 0.0949, 0.0625, 0.0684, 0.0927, 0.5433, 0.4854, 0.0984, 0.1330,\n",
       "        0.0923, 0.5203, 0.0138, 0.0967, 0.1650, 0.2655, 0.1632, 0.5373, 0.0156,\n",
       "        0.2086, 0.1533, 0.0811, 0.0150, 0.0162, 0.1180, 0.4647, 0.4829, 0.4697,\n",
       "        0.0139, 0.0951, 0.0154, 0.0158, 0.5088, 0.1817, 0.5261, 0.1300, 0.5501,\n",
       "        0.4999, 0.1449, 0.0863, 0.0150, 0.5222, 0.1705, 0.3236, 0.0137, 0.2896,\n",
       "        0.0965, 0.1667, 0.3585, 0.1282, 0.0128, 0.0619, 0.0144, 0.3251, 0.0158,\n",
       "        0.0955, 0.0626, 0.5359, 0.0153, 0.0770, 0.1255, 0.1592, 0.0931, 0.0693,\n",
       "        0.0133, 0.1323, 0.0643, 0.5949, 0.0163, 0.0929, 0.0838, 0.0130, 0.5209,\n",
       "        0.1766, 0.0141, 0.1111, 0.5224, 0.0671, 0.4897, 0.4389, 0.0165, 0.5114,\n",
       "        0.1433, 0.1408, 0.0156, 0.6237, 0.1262, 0.4698, 0.0863, 0.6347, 0.4830,\n",
       "        0.0149, 0.2617, 0.1860, 0.0936, 0.0918, 0.0160, 0.6311, 0.1496, 0.3509,\n",
       "        0.5142, 0.5433, 0.1860, 0.1936, 0.3226, 0.0896, 0.0915, 0.3270, 0.3122,\n",
       "        0.0153, 0.4706, 0.5367, 0.4981, 0.0603, 0.3232, 0.0910, 0.5819, 0.2681,\n",
       "        0.0149, 0.1727, 0.4792, 0.4904, 0.0164, 0.0929, 0.0645, 0.2877, 0.5330,\n",
       "        0.4571, 0.0140, 0.0153, 0.0845, 0.5776, 0.1210, 0.0155, 0.5088, 0.3173,\n",
       "        0.0806, 0.0155, 0.0984, 0.0890, 0.1929, 0.1240, 0.0139, 0.1932, 0.5105,\n",
       "        0.0154, 0.5031, 0.1244, 0.0931, 0.0141, 0.0144, 0.0138, 0.6542, 0.2885,\n",
       "        0.1549, 0.5390, 0.5476, 0.0164, 0.4713, 0.0871, 0.0141, 0.1494, 0.1425,\n",
       "        0.2069, 0.0157, 0.1608, 0.1289, 0.0150, 0.0140, 0.1890, 0.3263, 0.1984,\n",
       "        0.1957, 0.3333, 0.0860, 0.0796, 0.0887, 0.2144, 0.1775, 0.5057, 0.5335,\n",
       "        0.5339, 0.6014, 0.5220, 0.1565, 0.0673, 0.2076, 0.2106, 0.5370, 0.0132,\n",
       "        0.1620, 0.0648, 0.1724, 0.3108, 0.1774, 0.1238, 0.5343, 0.5233, 0.1122,\n",
       "        0.0643, 0.1920, 0.0847, 0.1400, 0.1495, 0.0933, 0.5188, 0.0960, 0.1289,\n",
       "        0.5051, 0.0153, 0.0637, 0.5131, 0.2737, 0.1569, 0.2066, 0.5161, 0.0640,\n",
       "        0.0158, 0.1703, 0.1928, 0.0804, 0.1249, 0.2145, 0.3345, 0.0921, 0.3248,\n",
       "        0.0129, 0.5136, 0.1433, 0.4909, 0.0162, 0.5241, 0.0152, 0.5323, 0.2006,\n",
       "        0.4776, 0.1305, 0.0133, 0.1461, 0.5876, 0.0874, 0.0152, 0.6049, 0.1558,\n",
       "        0.3248, 0.1153, 0.0153, 0.0157, 0.1191, 0.0152, 0.0976, 0.0928, 0.0597,\n",
       "        0.4840, 0.0158, 0.0155, 0.5849], device='cuda:0',\n",
       "       grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:17:33.019031Z",
     "start_time": "2020-07-22T17:17:32.991490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:18:07.542463Z",
     "start_time": "2020-07-22T17:18:07.481528Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d242af1318ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/imagineIOT/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/imagineIOT/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    932\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/imagineIOT/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/imagineIOT/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "loss_func(reward.view(-1), batch['reward'].long().to(params['device']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-22T17:37:06.633733Z",
     "start_time": "2020-07-22T17:37:06.601806Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You set the color of first light bulb to blue',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You turned on the first plug',\n",
       " 'You turned on the first plug',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You turned off the first light bulb',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You turned on the first light bulb',\n",
       " 'You turned off the first plug',\n",
       " 'You turned on the first plug',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You turned on the first light bulb',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'You turned on the first plug',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You turned off the first plug',\n",
       " 'You turned off the first light bulb',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'You turned off the first plug',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You turned off the first light bulb',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'You turned off the first plug',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You turned on the first plug',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You turned off the first plug',\n",
       " 'You turned on the first light bulb',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You turned on the first plug',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You turned off the first light bulb',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You turned off the first light bulb',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'You turned on the first plug',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'You turned off the first plug',\n",
       " 'You turned on the first light bulb',\n",
       " 'You turned on the first plug',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You turned off the first light bulb',\n",
       " 'You turned on the first plug',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You turned off the first light bulb',\n",
       " 'You turned on the first plug',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You turned on the first light bulb',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You turned on the first plug',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You turned on the first plug',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'You turned off the first plug',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You turned off the first plug',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'You turned on the first plug',\n",
       " 'You turned on the first plug',\n",
       " 'You turned on the first plug',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You turned off the first light bulb',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You set the color of first light bulb to red',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You turned off the first plug',\n",
       " 'You turned off the first light bulb',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'You turned on the first light bulb',\n",
       " 'You set the color of first light bulb to yellow',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You turned on the first light bulb',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You turned on the first light bulb',\n",
       " 'You set the color of first light bulb to blue',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You turned off the first light bulb',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'You made the light of first light bulb warmer',\n",
       " 'You turned on the first plug',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You turned off the first plug',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You turned on the first plug',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'You turned on the first plug',\n",
       " 'You set the color of first light bulb to pink',\n",
       " 'You turned off the first light bulb',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You set the color of first light bulb to orange',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'The luminosity of first light bulb is now high',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now very low',\n",
       " 'You set the color of first light bulb to purple',\n",
       " 'You set the color of first light bulb to green',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You made the light of first light bulb colder',\n",
       " 'You decreased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now very high',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'The luminosity of first light bulb is now low',\n",
       " 'The luminosity of first light bulb is now average',\n",
       " 'You turned off the first plug',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'You increased the luminosity of first light bulb',\n",
       " 'The luminosity of first light bulb is now very low']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
